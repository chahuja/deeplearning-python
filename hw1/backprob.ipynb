{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BackProb in Multilayer Perceptron NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import pdb\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = np.matrix(np.genfromtxt('digitstrain.txt', delimiter=','))\n",
    "np.random.shuffle(train) ## Shuffle to improve convergence\n",
    "test = np.matrix(np.genfromtxt('digitstest.txt', delimiter=','))\n",
    "val = np.matrix(np.genfromtxt('digitsvalid.txt', delimiter=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for training, validation and testing\n",
    "NUM_CLASSES = 10 ## hardcoded\n",
    "## converting ground truths to one-hot encoding\n",
    "def cat_to_one_hot(vec):\n",
    "  vec = vec.astype(int) ## changing type to int as these are indices for the one-hot vector\n",
    "  return np.matrix(np.eye(NUM_CLASSES)[vec])\n",
    "\n",
    "train_gt = cat_to_one_hot(train[:,-1])\n",
    "test_gt = cat_to_one_hot(test[:,-1])\n",
    "val_gt = cat_to_one_hot(val[:,-1])\n",
    "\n",
    "train = train[:,:-1]\n",
    "test = test[:,:-1]\n",
    "val = val[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Image of a training input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the image \n",
    "## The data is in row-major format\n",
    "def plot_image(train):\n",
    "  plt.imshow(train[0].reshape((28,28)))\n",
    "## The image is squeezed row-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic loss/activation functions and their gradients which are codenamed \"inv\"\n",
    "# These functions are defined with input as numpy.matrix format.\n",
    "# things may go haywire if numpy.array is used \n",
    "# even though numpy.matrix inherits properties from numpy.array\n",
    "def cross_entropy_loss(vec, gt):\n",
    "  return -np.multiply(gt,np.log(vec)).sum()/vec.shape[0] ## take the average\n",
    "\n",
    "def classification_error(vec,gt):\n",
    "  #pdb.set_trace()\n",
    "  dif = abs(np.argmax(vec,axis=1)-np.argmax(gt,axis=1))\n",
    "  err = np.ceil((1.0*dif)/max(dif))\n",
    "  relative_err = (1.0*err.sum())/len(err)\n",
    "  return relative_err*100\n",
    "\n",
    "def sigmoid(mat):\n",
    "  return 1./(1+ np.exp(-mat))\n",
    "\n",
    "def inv_sigmoid(mat):\n",
    "  return np.multiply(sigmoid(mat),(1-sigmoid(mat)))\n",
    "\n",
    "def softmax(vec):\n",
    "  return np.concatenate(\n",
    "    tuple([np.exp(vec[i,:])*1./np.exp(vec[i,:]).sum() for i in range(vec.shape[0])])\n",
    "    , axis = 0)\n",
    "\n",
    "def inv_softmax_with_loss(vec, gt):\n",
    "  # gt - ground truth in one hot vector format\n",
    "  if (vec.shape != gt.shape):\n",
    "    raise Exception(\"Prediction and Expected Values must have the same dimensions\")\n",
    "    \n",
    "  return softmax(vec) - gt\n",
    "\n",
    "def relu(value):\n",
    "  if (value<=0):\n",
    "    return 0\n",
    "  else:\n",
    "    return value\n",
    "\n",
    "def dropout_mask(prob, dim):\n",
    "  if (prob>1 or prob<0):\n",
    "    raise Exception(\"Probability has to be between 0 and 1\")\n",
    "\n",
    "  ## in the forward pass, the mask has the probability of the selection of the node\n",
    "  ## while in the backward pass, the mask has value 1 for \"prob\" fraction of the elements\n",
    "  forward_mask = np.matrix(np.ones((1,dim)) * prob * 1.0)\n",
    "  forward_mask = np.concatenate((forward_mask, np.matrix([1])), axis=1)\n",
    "  #backprob_mask = np.matrix([1 for _ in range(int(np.ceil(prob*dim)))]+ [0 for _ in range(int(dim - np.ceil(prob*dim)))])\n",
    "  backprob_mask = np.abs(np.ceil(np.random.uniform(size=(1,dim))-prob))\n",
    "  np.random.shuffle(backprob_mask.T) ## random placement of ones\n",
    "  backprob_mask = np.concatenate((backprob_mask,np.matrix([1])), axis=1)\n",
    "  return forward_mask, backprob_mask\n",
    "\n",
    "def copy_list(a):\n",
    "  return [a[i].copy() for i in range(len(a))]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualizing filters\n",
    "def vis(model, save_name):\n",
    "  gs = gridspec.GridSpec(10,10,top=1., bottom=0., right=1., left=0., hspace=0., wspace=0.)\n",
    "  for g,count in zip(gs,range(100)):\n",
    "    ax = plt.subplot(g)\n",
    "    ax.imshow(model.weights[0][:-1,count].reshape((28,28)))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "  plt.savefig(save_name + '_vis.png')\n",
    "\n",
    "def plot_cce(model, save_name):  \n",
    "  train_plt = plt.plot(range(len(model.hist.train_loss)),model.hist.train_loss, 'r--', label='Train')\n",
    "  val_plt = plt.plot(range(len(model.hist.val_loss)),model.hist.val_loss, 'g-', label=\"Val\")\n",
    "  plt.xlabel('No. of Epochs')\n",
    "  plt.ylabel('mean(Entropy Loss)')\n",
    "  plt.savefig(save_name+'.png')\n",
    "\n",
    "def plot_err(model, save_name):  \n",
    "  train_plt = plt.plot(range(len(model.hist.train_loss)),model.hist.train_class_loss, 'r--', label='Train')\n",
    "  val_plt = plt.plot(range(len(model.hist.val_loss)),model.hist.val_class_loss, 'g-', label=\"Val\")\n",
    "  plt.xlabel('No. of Epochs')\n",
    "  plt.ylabel('Classification Error')\n",
    "  plt.savefig(save_name+'_err.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_check(model, X, gt, ep = 0.0001):\n",
    "  ## Estimate the gradients using first principles\n",
    "  grad_W_cap = list()\n",
    "  for layer in range(model.num_layers):\n",
    "    ## add a zero matrix which will be updated with estimated gradients\n",
    "    grad_W_cap.append(np.matrix(np.zeros_like(model.weights[layer])))\n",
    "    for index, _ in tqdm(np.ndenumerate(model.weights[layer])):\n",
    "      ## Increase the model weights by epsilon (a.k.a ep)\n",
    "      ## and calculate the loss function L(w+ep)\n",
    "      old_weight = model.weights[layer][index]\n",
    "      model.weights[layer][index] += ep\n",
    "      L_x_plus_ep = cross_entropy_loss(model.forward(X,activation=True),gt)\n",
    "      model.weights[layer][index] = old_weight\n",
    "      ## Decrease the model weights by epsilon (a.k.a ep)\n",
    "      ## and calculate the loss function L(w-ep)\n",
    "      model.weights[layer][index] -= ep\n",
    "      L_x_minus_ep = cross_entropy_loss(model.forward(X,activation=True),gt)\n",
    "      model.weights[layer][index] = old_weight\n",
    "      \n",
    "      ## Update the estimated gradient in the grad_W_cap list\n",
    "      ## We use log wise addition as the denominater is fairly small \n",
    "      ## which might result in precision errors\n",
    "      grad_W_cap[layer][index] = np.exp(np.log(L_x_plus_ep - L_x_minus_ep) - np.log(2*ep))\n",
    "  return grad_W_cap\n",
    "  ## Plot Heatmaps of (grad_W  - grad_W_cap)\n",
    "  for layer in range(model.num_layers):\n",
    "    sb.heatmap(model.grad_W[layer] - grad_W_cap[layer])\n",
    "  pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## gradient check\n",
    "def grad_check(model):\n",
    "  layer = 0\n",
    "  sb.heatmap(model.grad_W[layer] - grad_W_cap[layer])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "  fl = open(filename,'wb')\n",
    "  pkl.dump(model,fl)\n",
    "  \n",
    "def load_model(filename):\n",
    "  fl = open(filename, 'rb')\n",
    "  return pkl.load(fl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss History Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class history(object):\n",
    "  def __init__(self):\n",
    "    self.train_loss = list()\n",
    "    self.val_loss = list()\n",
    "    self.train_class_loss = list()\n",
    "    self.val_class_loss = list()\n",
    "\n",
    "  def add(self,train_loss, val_loss,train_class_loss,val_class_loss):\n",
    "    self.train_loss.append(train_loss)\n",
    "    self.val_loss.append(val_loss)\n",
    "    self.train_class_loss.append(train_class_loss)\n",
    "    self.val_class_loss.append(val_class_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class NN\n",
    "```sh\n",
    "graph - [num_nodes(int), num_nodes(int), ...]\n",
    "each layer ends with sigmoid\n",
    "graph ends with softmax and crossentropy loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "  def __init__(self,graph):\n",
    "    self.graph = graph\n",
    "    self.weights = list()\n",
    "    self.weights_optimal = list() ## to store the optimal weights\n",
    "    self.hist = history()\n",
    "    self.v = list() ## momentum\n",
    "    \n",
    "    prev_dim = -1\n",
    "    for dim in self.graph:\n",
    "        if (prev_dim > 0):\n",
    "          # set the high and low bounds for the random initialization \n",
    "          # based on glorot's rule\n",
    "          high = np.sqrt(6.0/(prev_dim + dim))\n",
    "          low = -high\n",
    "          # prev_dim + 1 to include a row for bias \n",
    "          self.weights.append(np.random.uniform(low=low,high=high,size=(prev_dim+1,dim)))\n",
    "          ## momentum has the same dimension as the weight matrices\n",
    "          self.v.append(np.matrix(np.zeros_like(self.weights[-1])))\n",
    "        prev_dim = dim\n",
    "    \n",
    "    self.num_layers = len(self.weights)\n",
    "    self.weights_optimal = copy_list(self.weights)\n",
    "\n",
    "  ## to restore optimal weights when restoring a model\n",
    "  def restore_optimal_weights(self):\n",
    "    self.weights = copy_list(self.weights_optimal)\n",
    "    \n",
    "  def plot(self):\n",
    "    #sb.pointplot(x=\"num_epochs\", y=\"entropy_loss\", data=[self.hist.train_loss,self.hist.val_loss]);\n",
    "    plt.plot(self.hist.train_loss,range(len(self.hist.train_loss)), 'r--')\n",
    "    plt.plot(self.hist.val_loss,range(len(self.hist.val_loss)), 'g+-')\n",
    "\n",
    "  def forward(self,X,limit = 0, activation = False, prob=1):\n",
    "    # We use the forward function to calculate the forward prop\n",
    "    ## while training the model, various segments of the forward prop are required\n",
    "    ## which need to be extracted at the right times\n",
    "    # \"limit\" and \"activation\" define the layers that have to be removed from the end\n",
    "    ## limit = 0 and activation=True implies complete calculation\n",
    "    ## limit = 0 and activation=False ignores the softmax function but calculates till last layer\n",
    "    ## limit = 1 and activation=True implies calculation till second-last layer w/ sigmoid\n",
    "    ## limit = 1 and activation=False implies calculation till second-last layer w/o sgimoid\n",
    "    ## now the limit belongs to [0,len(NN.weights)-1], hence with other values it will give junk\n",
    "    ## also for dropout we define a parameter called \"prob\" which defines the fraction of the \n",
    "    ## nodes to be active during training\n",
    "    # raise havoc if the limits are not correct\n",
    "    if(limit > self.num_layers or limit < 0):\n",
    "      raise Exception(\"Limits of the network are out of bounds\")\n",
    "    # add a column of ones to take care of the bias only if we do not need the input layer\n",
    "    if((limit == self.num_layers and activation == True) or limit < self.num_layers):\n",
    "      X = np.concatenate((X,np.ones_like(X[:,0])), axis = 1)\n",
    "    # Converting input to matrix form\n",
    "    X = np.mat(X)\n",
    "    \n",
    "    # limits\n",
    "    ## 0 -> -1\n",
    "    ## 1 -> -1\n",
    "    ## 2 -> -2 and so on which generalizes to relu(limit-1)-1\n",
    "    limits = range(self.num_layers-relu(limit-1)-1)\n",
    "    for k in limits:\n",
    "      X = X*np.mat(self.weights[k]) ## Linear\n",
    "      ## if activation=False skip the non-linear part\n",
    "      ## if k is the final element of the limits only then break \n",
    "      ## if limit is not 0, then all the layers must be taken into consideration\n",
    "      if (activation == False and k == limits[-1] and limit != 0):\n",
    "        break\n",
    "      X = sigmoid(X) ## Non-Linear\n",
    "      X = np.concatenate((X,np.ones_like(X[:,0])), axis = 1)\n",
    "      forward_mask, _ = dropout_mask(prob=prob, dim=X.shape[1]-1) ## subtract 1 due to the bias\n",
    "      X = np.multiply(X, forward_mask)\n",
    "      \n",
    "    # if the limit is anything apart the last layer, return the calculated values\n",
    "    if (limit > 0):\n",
    "      return X\n",
    "    \n",
    "    X = X*np.mat(self.weights[-1]) ## Linear\n",
    "    if (activation == True):\n",
    "      return softmax(X) ## Non-Linear\n",
    "    else: \n",
    "      return X\n",
    "  \n",
    "  # implements one iteration of backpropogation through the network\n",
    "  ## it has to use forward() to estimate gradient values at each layer\n",
    "  ## the gradient values for every matrix are returned as a list of matrices\n",
    "  ## which will be used for optimizing the model via stochastic/batch/mini-batch descent\n",
    "  # Note: the gradients are calculated based on the original set of weights provided as X\n",
    "  ## and all the weights ar updated at the end of a complete back prob\n",
    "  # -------------------------------------------------------------------------------\n",
    "  # Variables\n",
    "  # ---------\n",
    "  # grad_W = gradients w.r.t. weights including bias\n",
    "  # grad_H = gradients w.r.t. hidden layers\n",
    "  # grad_A = gradients w.r.t. pre-activation layer\n",
    "  # All of these variables are lists and updated as the backprob traverses through \n",
    "  ## different layers\n",
    "  # -------------------------------------------------------------------------------\n",
    "  def backprob(self, X, gt, prob=1, lam=0):\n",
    "    self.grad_W = list()\n",
    "    grad_H = list()\n",
    "    grad_A = list()\n",
    "    \n",
    "    ## initializing first gradient\n",
    "    grad_A.append(inv_softmax_with_loss(self.forward(X, prob=prob), gt))\n",
    "    for k in range(self.num_layers):\n",
    "      ## Calculating the forward pass\n",
    "      forward_pass = self.forward(X, k+1, True, prob=prob)\n",
    "      _, backprob_mask = dropout_mask(prob=prob, dim=forward_pass.shape[1]-1) ## subtract 1 due to the bias\n",
    "      #masked_input = forward_pass\n",
    "      #backprob_mask = np.matrix(np.ones_like(forward_pass))\n",
    "      masked_input = np.multiply(forward_pass,backprob_mask)\n",
    "      mod_w = np.abs(self.weights[self.num_layers-k-1])\n",
    "      mod_w[-1] = 0 ## as the bias should not be regularized\n",
    "      self.grad_W.append(masked_input.T * grad_A[k] - lam*mod_w)\n",
    "      ## We do not need the bias layer to update the gradients so we do not keep it\n",
    "      grad_H.append(self.weights[self.num_layers-k-1][:-1,:] * grad_A[k].T) \n",
    "      grad_A.append(np.multiply(grad_H[k].T,inv_sigmoid(self.forward(X,k+1, prob=prob))))\n",
    "\n",
    "    ## reverse the list of gradients as they were stored in the order of backprob\n",
    "    self.grad_W.reverse()\n",
    "    \n",
    "  ## gradient updates which requires learning rate(lr) and momentum(mm)\n",
    "  def grad_descent(self, X, gt, lr=0.1, mm=0, prob=1, lam=0, optimal_limit=False):\n",
    "    if(not optimal_limit):\n",
    "      self.weights_optimal = copy_list(self.weights)\n",
    "      \n",
    "    ## calculating gradients\n",
    "    self.backprob(X, gt, prob, lam)\n",
    "    ## updating gradients\n",
    "    for k in range(self.num_layers):\n",
    "      self.v[k] = self.v[k] * mm - lr * self.grad_W[k]\n",
    "      self.weights[k] = self.weights[k] + self.v[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sgd_train(args):\n",
    "  model = NN(args.graph)\n",
    "  # SGD\n",
    "  prob = args.prob ## Dropout coeffiecient\n",
    "  lam = args.lam ## regularization coeffiecient\n",
    "  lr = args.lr\n",
    "  mm = args.mm \n",
    "  optimal_limit = False\n",
    "  save_name = \"%s_lr%.3f_mm%.3f_lam%.3f_dropout%.1f\" %(args.save_dir,args.lr,args.mm,args.lam,args.prob)\n",
    "  save_name = os.path.join(args.save_dir,save_name)\n",
    "  for i in tqdm(range(args.num_epochs)):\n",
    "    for j in range(train.shape[0]):\n",
    "      model.grad_descent(train[j], train_gt[j], lr=lr, mm=mm, prob=prob, lam=lam, optimal_limit=optimal_limit)\n",
    "    train_pred = model.forward(train,activation=True, prob=prob)\n",
    "    val_pred = model.forward(val,activation=True, prob=prob)\n",
    "    train_class_loss = classification_error(train_pred,train_gt)\n",
    "    val_class_loss = classification_error(val_pred,val_gt)\n",
    "    model.hist.add(cross_entropy_loss(train_pred, train_gt),\n",
    "                  cross_entropy_loss(val_pred, val_gt), train_class_loss, val_class_loss)\n",
    "    \n",
    "    try:\n",
    "      if (model.hist.val_loss[-2] < model.hist.val_loss[-1]):\n",
    "        optimal_limit = True\n",
    "      else:\n",
    "        optimal_limit = False\n",
    "    except:\n",
    "      pass\n",
    "      \n",
    "    print(\"train error: %f, %f\") %(model.hist.train_loss[-1],model.hist.train_class_loss[-1])\n",
    "    print(\"val error: %f, %f\") %(model.hist.val_loss[-1],model.hist.val_class_loss[-1])\n",
    "    save_model(model,save_name+'.p')\n",
    "    \n",
    "  ## Error vs Epochs\n",
    "  plot_cce(model, save_name)\n",
    "  plot_err(model, save_name)\n",
    "  vis(model, save_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--save_dir', type=str, default='save',\n",
    "                        help='directory to store checkpointed models')\n",
    "    parser.add_argument('--graph', type=int ,nargs='+', default=[784,100,10],\n",
    "                        help='Structure of the NN') \n",
    "    parser.add_argument('--prob', type=float, default=1,\n",
    "                        help='dropout coefficients')\n",
    "    parser.add_argument('--lam', type=float, default=0,\n",
    "                        help='regularizing coefficient')\n",
    "    parser.add_argument('--lr', type=float, default=0.01,\n",
    "                        help='learning rate')\n",
    "    parser.add_argument('--mm', type=float, default=0,\n",
    "                        help='momentum')\n",
    "    parser.add_argument('--num_epochs', type=int, default=200, \n",
    "                        help='number of epochs')\n",
    "    parser.add_argument('--early_stopping', type=bool, default=False,\n",
    "                        help='If you want to perform early stopping')\n",
    "    args = parser.parse_args()\n",
    "    try:\n",
    "      os.makedirs(args.save_dir)\n",
    "    except:\n",
    "      pass\n",
    "    sgd_train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sys.argv=['train.py',\n",
    "         '--save_dir','model',\n",
    "         '--graph', '784', '100', '10',\n",
    "         '--prob', '0.5',\n",
    "         '--lam', '0',\n",
    "         '--lr', '0.01',\n",
    "         '--mm', '0.5',\n",
    "         '--num_epochs', '10',\n",
    "         '--early_stopping', 'False']\n",
    "if __name__==\"__main__\":\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depreciated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Depreciated\n",
    "def batch_train():\n",
    "  # Batch Training\n",
    "  for i in tqdm(range(200)):\n",
    "    #sample = list(np.floor(np.random.uniform(high=2999,low=0,size=(1000,))).astype(int))\n",
    "    #strain = np.matrix(np.array(train)[sample])\n",
    "    #strain_gt = np.matrix(np.array(train_gt)[sample])\n",
    "    model.grad_descent(train, train_gt, lr = 0.001, prob =1)\n",
    "    #pdb.set_trace()\n",
    "    train_pred = model.forward(train,activation=True, prob=1)\n",
    "    val_pred = model.forward(val,activation=True, prob=1)\n",
    "    model.hist.add(cross_entropy_loss(train_pred, train_gt),\n",
    "                  cross_entropy_loss(val_pred, val_gt))\n",
    "    print(\"train error: %f, %f\") %(model.hist.train_loss[-1],classification_error(train_pred,train_gt))\n",
    "    print(\"val error: %f, %f\") %(model.hist.val_loss[-1],classification_error(val_pred,val_gt))\n",
    "    #print(model.weights[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
