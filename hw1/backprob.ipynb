{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- write gradient check\n",
    "    - also check the gradient values of the weights. Are the weights chaging?\n",
    "- classification error is not working\n",
    "- and then start testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chahuja/env/tf/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import pdb\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = np.matrix(np.genfromtxt('digitstrain.txt', delimiter=','))\n",
    "test = np.matrix(np.genfromtxt('digitstest.txt', delimiter=','))\n",
    "val = np.matrix(np.genfromtxt('digitsvalid.txt', delimiter=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for training, validation and testing\n",
    "NUM_CLASSES = 10 ## hardcoded\n",
    "## converting ground truths to one-hot encoding\n",
    "def cat_to_one_hot(vec):\n",
    "  vec = vec.astype(int) ## changing type to int as these are indices for the one-hot vector\n",
    "  return np.matrix(np.eye(NUM_CLASSES)[vec])\n",
    "\n",
    "train_gt = cat_to_one_hot(train[:,-1])\n",
    "test_gt = cat_to_one_hot(test[:,-1])\n",
    "val_gt = cat_to_one_hot(val[:,-1])\n",
    "\n",
    "train = train[:,:-1]\n",
    "test = test[:,:-1]\n",
    "val = val[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Image of a training input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fd91528e250>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD/CAYAAAA+CADKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd9296ccf50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting the image\n",
    "plt.imshow(train[0].reshape((28,28)))\n",
    "## The image is squeezed row-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class description\n",
    "```sh\n",
    "graph - [num_nodes(int), num_nodes(int), ...]\n",
    "each layer ends with sigmoid\n",
    "graph ends with softmax and crossentropy loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic loss/activation functions and their gradients which are codenamed \"inv\"\n",
    "# These functions are defined with input as numpy.matrix format.\n",
    "# things may go haywire if numpy.array is used \n",
    "# even though numpy.matrix inherits properties from numpy.array\n",
    "def cross_entropy_loss(vec, gt):\n",
    "  return -np.multiply(gt,np.log(vec)).sum()/vec.shape[0] ## take the average\n",
    "\n",
    "def classification_error(vec,gt):\n",
    "  #pdb.set_trace()\n",
    "  dif = abs(np.argmax(vec,axis=1)-np.argmax(gt,axis=1))\n",
    "  err = np.ceil((1.0*dif)/max(dif))\n",
    "  relative_err = (1.0*err.sum())/len(err)\n",
    "  return relative_err*100\n",
    "\n",
    "def sigmoid(mat):\n",
    "  return 1./(1+ np.exp(-mat))\n",
    "\n",
    "def inv_sigmoid(mat):\n",
    "  return np.multiply(sigmoid(mat),(1-sigmoid(mat)))\n",
    "\n",
    "def softmax(vec):\n",
    "  return np.concatenate(\n",
    "    tuple([np.exp(vec[i,:])*1./np.exp(vec[i,:]).sum() for i in range(vec.shape[0])])\n",
    "    , axis = 0)\n",
    "\n",
    "def inv_softmax_with_loss(vec, gt):\n",
    "  # gt - ground truth in one hot vector format\n",
    "  if (vec.shape != gt.shape):\n",
    "    raise Exception(\"Prediction and Expected Values must have the same dimensions\")\n",
    "    \n",
    "  return softmax(vec) - gt\n",
    "\n",
    "def relu(value):\n",
    "  if (value<=0):\n",
    "    return 0\n",
    "  else:\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class history(object):\n",
    "  def __init__(self):\n",
    "    self.train_loss = list()\n",
    "    self.val_loss = list()\n",
    "\n",
    "  def add(self,train_loss, val_loss):\n",
    "    self.train_loss.append(train_loss)\n",
    "    self.val_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "  def __init__(self,graph):\n",
    "    self.graph = graph\n",
    "    self.weights = list()\n",
    "    self.hist = history()\n",
    "    self.v = list() ## momentum\n",
    "    \n",
    "    prev_dim = -1\n",
    "    for dim in self.graph:\n",
    "        if (prev_dim > 0):\n",
    "          # set the high and low bounds for the random initialization \n",
    "          # based on glorot's rule\n",
    "          high = np.sqrt(6.0/(prev_dim + dim))\n",
    "          low = -high\n",
    "          # prev_dim + 1 to include a row for bias \n",
    "          self.weights.append(np.random.uniform(low=low,high=high,size=(prev_dim+1,dim)))\n",
    "          ## momentum has the same dimension as the weight matrices\n",
    "          self.v.append(np.matrix(np.zeros_like(self.weights[-1])))\n",
    "        prev_dim = dim\n",
    "    \n",
    "    self.num_layers = len(self.weights)\n",
    "\n",
    "  def plot(self):\n",
    "    #sb.pointplot(x=\"num_epochs\", y=\"entropy_loss\", data=[self.hist.train_loss,self.hist.val_loss]);\n",
    "    plt.plot(self.hist.train_loss,range(len(self.hist.train_loss)), 'r--')\n",
    "    plt.plot(self.hist.val_loss,range(len(self.hist.val_loss)), 'g+-')\n",
    "\n",
    "  def forward(self,X,limit = 0, activation = False):\n",
    "    # We use the forward function to calculate the forward prop\n",
    "    ## while training the model, various segments of the forward prop are required\n",
    "    ## which need to be extracted at the right times\n",
    "    # \"limit\" and \"activation\" define the layers that have to be removed from the end\n",
    "    ## limit = 0 and activation=True implies complete calculation\n",
    "    ## limit = 0 and activation=False ignores the softmax function but calculates till last layer\n",
    "    ## limit = 1 and activation=True implies calculation till second-last layer w/ sigmoid\n",
    "    ## limit = 1 and activation=False implies calculation till second-last layer w/o sgimoid\n",
    "    ## now the limit belongs to [0,len(NN.weights)-1], hence with other values it will give junk\n",
    "    # raise havoc if the limits are not correct\n",
    "    if(limit > self.num_layers or limit < 0):\n",
    "      raise Exception(\"Limits of the network are out of bounds\")\n",
    "    # add a column of ones to take care of the bias only if we do not need the input layer\n",
    "    if((limit == self.num_layers and activation == True) or limit < self.num_layers):\n",
    "      X = np.concatenate((X,np.ones_like(X[:,0])), axis = 1)\n",
    "    # Converting input to matrix form\n",
    "    X = np.mat(X)\n",
    "    \n",
    "    # limits\n",
    "    ## 0 -> -1\n",
    "    ## 1 -> -1\n",
    "    ## 2 -> -2 and so on which generalizes to relu(limit-1)-1\n",
    "    limits = range(self.num_layers-relu(limit-1)-1)\n",
    "    #print limits\n",
    "    for k in limits:\n",
    "      X = X*np.mat(self.weights[k]) ## Linear\n",
    "      ## if activation=False skip the non-linear part\n",
    "      ## if k is the final element of the limits only then break \n",
    "      ## if limit is not 0, then all the layers must be taken into consideration\n",
    "      if (activation == False and k == limits[-1] and limit != 0):\n",
    "        break\n",
    "      X = sigmoid(X) ## Non-Linear\n",
    "      X = np.concatenate((X,np.ones_like(X[:,0])), axis = 1)\n",
    "    \n",
    "    # if the limit is anything apart the last layer, return the calculated values\n",
    "    if (limit > 0):\n",
    "      return X\n",
    "    \n",
    "    X = X*np.mat(self.weights[-1]) ## Linear\n",
    "    if (activation == True):\n",
    "      return softmax(X) ## Non-Linear\n",
    "    else: \n",
    "      return X\n",
    "  \n",
    "  # implements one iteration of backpropogation through the network\n",
    "  ## it has to use forward() to estimate gradient values at each layer\n",
    "  ## the gradient values for every matrix are returned as a list of matrices\n",
    "  ## which will be used for optimizing the model via stochastic/batch/mini-batch descent\n",
    "  # Note: the gradients are calculated based on the original set of weights provided as X\n",
    "  ## and all the weights ar updated at the end of a complete back prob\n",
    "  # -------------------------------------------------------------------------------\n",
    "  # Variables\n",
    "  # ---------\n",
    "  # grad_W = gradients w.r.t. weights including bias\n",
    "  # grad_H = gradients w.r.t. hidden layers\n",
    "  # grad_A = gradients w.r.t. pre-activation layer\n",
    "  # All of these variables are lists and updated as the backprob traverses through \n",
    "  ## different layers\n",
    "  # -------------------------------------------------------------------------------\n",
    "  def backprob(self, X, gt):\n",
    "    self.grad_W = list()\n",
    "    grad_H = list()\n",
    "    grad_A = list()\n",
    "    \n",
    "    ## initializing first gradient\n",
    "    grad_A.append(inv_softmax_with_loss(self.forward(X), gt))\n",
    "    for k in range(self.num_layers):\n",
    "      self.grad_W.append(self.forward(X, k+1, True).T * grad_A[k])\n",
    "      ## We do not need the bias layer to update the gradients so we do not keep it\n",
    "      grad_H.append(self.weights[self.num_layers-k-1][:-1,:] * grad_A[k].T) \n",
    "      grad_A.append(np.multiply(grad_H[k].T,inv_sigmoid(self.forward(X,k+1))))\n",
    "\n",
    "    ## reverse the list of gradients as they were stored in the order of backprob\n",
    "    self.grad_W.reverse()\n",
    "    \n",
    "  ## gradient updates which requires learning rate(lr) and momentum(mm)\n",
    "  def grad_descent(self, X, gt, lr=0.1, mm=0):\n",
    "    ## calculating gradients\n",
    "    self.backprob(X, gt)\n",
    "    ## updating gradients\n",
    "    for k in range(self.num_layers):\n",
    "      self.v[k] = self.v[k] * mm - lr * self.grad_W[k]\n",
    "      self.weights[k] = self.weights[k] + self.v[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "  fl = open(filename,'wb')\n",
    "  pkl.dump(model,fl)\n",
    "  \n",
    "def load_model(filename):\n",
    "  fl = open(filename, 'rb')\n",
    "  return pkl.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 100)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NN([784,100, 10])\n",
    "#model.forward(train[0:2,:-1],0, True).shape\n",
    "#model.forward(train[0:2,:-1],1, True).shape\n",
    "model.forward(train[0:2],1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-13-31c190bc2a61>(7)<module>()\n",
      "-> train_pred = model.forward(train,activation=True)\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(1000)):\n",
    "  #sample = list(np.floor(np.random.uniform(high=2999,low=0,size=(1000,))).astype(int))\n",
    "  #strain = np.matrix(np.array(train)[sample])\n",
    "  #strain_gt = np.matrix(np.array(train_gt)[sample])\n",
    "  model.grad_descent(train, train_gt, lr = 0.001)\n",
    "  pdb.set_trace()\n",
    "  train_pred = model.forward(train,activation=True)\n",
    "  val_pred = model.forward(val,activation=True)\n",
    "  model.hist.add(cross_entropy_loss(train_pred, train_gt),\n",
    "                cross_entropy_loss(val_pred, val_gt))\n",
    "  print(\"train error: %f, %f\") %(model.hist.train_loss[-1],classification_error(train_pred,train_gt))\n",
    "  print(\"val error: %f, %f\") %(model.hist.val_loss[-1],classification_error(val_pred,val_gt))\n",
    "  #print(model.weights[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    train_plt = plt.plot(range(len(model.hist.train_loss)),model.hist.train_loss, 'r--', label='Train')\n",
    "    val_plt = plt.plot(range(len(model.hist.val_loss)),model.hist.val_loss, 'g-', label=\"Val\")\n",
    "    plt.xlabel('No. of Epochs')\n",
    "    plt.ylabel('mean(Entropy Loss)')\n",
    "    #plt.legend([train_plt,val_plt])\n",
    "    #plt.axis([100,600, 0, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "classification_error(model.forward(val, activation=True),val_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.grad_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#np.matrix(np.zeros(10,1)[train[0,-1]])\n",
    "b = np.zeros((10,1))\n",
    "b[int(train[0,-1])] = 1\n",
    "b = np.matrix(b)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sb.distplot(model.forward(train[0:2,:-1],2)[0])\n",
    "model.num_layers\n",
    "#model.forward(train[0:2,:-1],2)[0,:-1] - train[0:1,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 2\n",
    "sb.distplot(model.weights[i].reshape((np.prod(model.weights[i].shape),)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.floor(np.random.uniform(high=2999,low=0,size=(100,))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a[[2,3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = list(np.floor(np.random.uniform(high=2999,low=0,size=(100,))).astype(int))\n",
    "np.array(train)[sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.matrix([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argmax(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
