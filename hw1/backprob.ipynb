{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "- and then start testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import pdb\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train = np.matrix(np.genfromtxt('digitstrain.txt', delimiter=','))\n",
    "np.random.shuffle(train) ## Shuffle to improve convergence\n",
    "test = np.matrix(np.genfromtxt('digitstest.txt', delimiter=','))\n",
    "val = np.matrix(np.genfromtxt('digitsvalid.txt', delimiter=','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare data for training, validation and testing\n",
    "NUM_CLASSES = 10 ## hardcoded\n",
    "## converting ground truths to one-hot encoding\n",
    "def cat_to_one_hot(vec):\n",
    "  vec = vec.astype(int) ## changing type to int as these are indices for the one-hot vector\n",
    "  return np.matrix(np.eye(NUM_CLASSES)[vec])\n",
    "\n",
    "train_gt = cat_to_one_hot(train[:,-1])\n",
    "test_gt = cat_to_one_hot(test[:,-1])\n",
    "val_gt = cat_to_one_hot(val[:,-1])\n",
    "\n",
    "train = train[:,:-1]\n",
    "test = test[:,:-1]\n",
    "val = val[:,:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Image of a training input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting the image\n",
    "plt.imshow(train[0].reshape((28,28)))\n",
    "## The image is squeezed row-wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic loss/activation functions and their gradients which are codenamed \"inv\"\n",
    "# These functions are defined with input as numpy.matrix format.\n",
    "# things may go haywire if numpy.array is used \n",
    "# even though numpy.matrix inherits properties from numpy.array\n",
    "def cross_entropy_loss(vec, gt):\n",
    "  return -np.multiply(gt,np.log(vec)).sum()/vec.shape[0] ## take the average\n",
    "\n",
    "def classification_error(vec,gt):\n",
    "  #pdb.set_trace()\n",
    "  dif = abs(np.argmax(vec,axis=1)-np.argmax(gt,axis=1))\n",
    "  err = np.ceil((1.0*dif)/max(dif))\n",
    "  relative_err = (1.0*err.sum())/len(err)\n",
    "  return relative_err*100\n",
    "\n",
    "def sigmoid(mat):\n",
    "  return 1./(1+ np.exp(-mat))\n",
    "\n",
    "def inv_sigmoid(mat):\n",
    "  return np.multiply(sigmoid(mat),(1-sigmoid(mat)))\n",
    "\n",
    "def softmax(vec):\n",
    "  return np.concatenate(\n",
    "    tuple([np.exp(vec[i,:])*1./np.exp(vec[i,:]).sum() for i in range(vec.shape[0])])\n",
    "    , axis = 0)\n",
    "\n",
    "def inv_softmax_with_loss(vec, gt):\n",
    "  # gt - ground truth in one hot vector format\n",
    "  if (vec.shape != gt.shape):\n",
    "    raise Exception(\"Prediction and Expected Values must have the same dimensions\")\n",
    "    \n",
    "  return softmax(vec) - gt\n",
    "\n",
    "def relu(value):\n",
    "  if (value<=0):\n",
    "    return 0\n",
    "  else:\n",
    "    return value\n",
    "\n",
    "def dropout_mask(prob, dim):\n",
    "  if (prob>1 or prob<0):\n",
    "    raise Exception(\"Probability has to be between 0 and 1\")\n",
    "\n",
    "  ## in the forward pass, the mask has the probability of the selection of the node\n",
    "  ## while in the backward pass, the mask has value 1 for \"prob\" fraction of the elements\n",
    "  forward_mask = np.matrix(np.ones((1,dim)) * prob * 1.0)\n",
    "  forward_mask = np.concatenate((forward_mask, np.matrix([1])), axis=1)\n",
    "  backprob_mask = np.matrix([1 for _ in range(int(np.ceil(prob*dim)))]\n",
    "                           + [0 for _ in range(int(dim - np.ceil(prob*dim)))])\n",
    "  np.random.shuffle(backprob_mask.T) ## random placement of ones\n",
    "  backprob_mask = np.concatenate((backprob_mask,np.matrix([1])), axis=1)\n",
    "  return forward_mask, backprob_mask\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic loss/activation functions and their gradients which are codenamed \"inv\"\n",
    "# These functions are defined with input as numpy.matrix format.\n",
    "# things may go haywire if numpy.array is used \n",
    "# even though numpy.matrix inherits properties from numpy.array\n",
    "def cross_entropy_loss(vec, gt):\n",
    "  return -np.multiply(gt,np.log(vec)).sum()/vec.shape[0] ## take the average\n",
    "\n",
    "def classification_error(vec,gt):\n",
    "  #pdb.set_trace()\n",
    "  dif = abs(np.argmax(vec,axis=1)-np.argmax(gt,axis=1))\n",
    "  err = np.ceil((1.0*dif)/max(dif))\n",
    "  relative_err = (1.0*err.sum())/len(err)\n",
    "  return relative_err*100\n",
    "\n",
    "def sigmoid(mat):\n",
    "  return 1./(1+ np.exp(-mat))\n",
    "\n",
    "def inv_sigmoid(mat):\n",
    "  return np.multiply(sigmoid(mat),(1-sigmoid(mat)))\n",
    "\n",
    "def softmax(vec):\n",
    "  return np.concatenate(\n",
    "    tuple([np.exp(vec[i,:])*1./np.exp(vec[i,:]).sum() for i in range(vec.shape[0])])\n",
    "    , axis = 0)\n",
    "\n",
    "def inv_softmax_with_loss(vec, gt):\n",
    "  # gt - ground truth in one hot vector format\n",
    "  if (vec.shape != gt.shape):\n",
    "    raise Exception(\"Prediction and Expected Values must have the same dimensions\")\n",
    "    \n",
    "  return softmax(vec) - gt\n",
    "\n",
    "def relu(value):\n",
    "  if (value<=0):\n",
    "    return 0\n",
    "  else:\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradient_check(model, X, gt, ep = 0.0001):\n",
    "  ## Estimate the gradients using first principles\n",
    "  grad_W_cap = list()\n",
    "  for layer in range(model.num_layers):\n",
    "    ## add a zero matrix which will be updated with estimated gradients\n",
    "    grad_W_cap.append(np.matrix(np.zeros_like(model.weights[layer])))\n",
    "    for index, _ in tqdm(np.ndenumerate(model.weights[layer])):\n",
    "      ## Increase the model weights by epsilon (a.k.a ep)\n",
    "      ## and calculate the loss function L(w+ep)\n",
    "      old_weight = model.weights[layer][index]\n",
    "      model.weights[layer][index] += ep\n",
    "      L_x_plus_ep = cross_entropy_loss(model.forward(X,activation=True),gt)\n",
    "      model.weights[layer][index] = old_weight\n",
    "      ## Decrease the model weights by epsilon (a.k.a ep)\n",
    "      ## and calculate the loss function L(w-ep)\n",
    "      model.weights[layer][index] -= ep\n",
    "      L_x_minus_ep = cross_entropy_loss(model.forward(X,activation=True),gt)\n",
    "      model.weights[layer][index] = old_weight\n",
    "      \n",
    "      ## Update the estimated gradient in the grad_W_cap list\n",
    "      ## We use log wise addition as the denominater is fairly small \n",
    "      ## which might result in precision errors\n",
    "      grad_W_cap[layer][index] = np.exp(np.log(L_x_plus_ep - L_x_minus_ep) - np.log(2*ep))\n",
    "  return grad_W_cap\n",
    "  ## Plot Heatmaps of (grad_W  - grad_W_cap)\n",
    "  for layer in range(model.num_layers):\n",
    "    sb.heatmap(model.grad_W[layer] - grad_W_cap[layer])\n",
    "  pdb.set_trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss History Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class history(object):\n",
    "  def __init__(self):\n",
    "    self.train_loss = list()\n",
    "    self.val_loss = list()\n",
    "\n",
    "  def add(self,train_loss, val_loss):\n",
    "    self.train_loss.append(train_loss)\n",
    "    self.val_loss.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class NN\n",
    "```sh\n",
    "graph - [num_nodes(int), num_nodes(int), ...]\n",
    "each layer ends with sigmoid\n",
    "graph ends with softmax and crossentropy loss\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NN(object):\n",
    "  def __init__(self,graph):\n",
    "    self.graph = graph\n",
    "    self.weights = list()\n",
    "    self.hist = history()\n",
    "    self.v = list() ## momentum\n",
    "    \n",
    "    prev_dim = -1\n",
    "    for dim in self.graph:\n",
    "        if (prev_dim > 0):\n",
    "          # set the high and low bounds for the random initialization \n",
    "          # based on glorot's rule\n",
    "          high = np.sqrt(6.0/(prev_dim + dim))\n",
    "          low = -high\n",
    "          # prev_dim + 1 to include a row for bias \n",
    "          self.weights.append(np.random.uniform(low=low,high=high,size=(prev_dim+1,dim)))\n",
    "          ## momentum has the same dimension as the weight matrices\n",
    "          self.v.append(np.matrix(np.zeros_like(self.weights[-1])))\n",
    "        prev_dim = dim\n",
    "    \n",
    "    self.num_layers = len(self.weights)\n",
    "\n",
    "  def plot(self):\n",
    "    #sb.pointplot(x=\"num_epochs\", y=\"entropy_loss\", data=[self.hist.train_loss,self.hist.val_loss]);\n",
    "    plt.plot(self.hist.train_loss,range(len(self.hist.train_loss)), 'r--')\n",
    "    plt.plot(self.hist.val_loss,range(len(self.hist.val_loss)), 'g+-')\n",
    "\n",
    "  def forward(self,X,limit = 0, activation = False, prob=1):\n",
    "    # We use the forward function to calculate the forward prop\n",
    "    ## while training the model, various segments of the forward prop are required\n",
    "    ## which need to be extracted at the right times\n",
    "    # \"limit\" and \"activation\" define the layers that have to be removed from the end\n",
    "    ## limit = 0 and activation=True implies complete calculation\n",
    "    ## limit = 0 and activation=False ignores the softmax function but calculates till last layer\n",
    "    ## limit = 1 and activation=True implies calculation till second-last layer w/ sigmoid\n",
    "    ## limit = 1 and activation=False implies calculation till second-last layer w/o sgimoid\n",
    "    ## now the limit belongs to [0,len(NN.weights)-1], hence with other values it will give junk\n",
    "    ## also for dropout we define a parameter called \"prob\" which defines the fraction of the \n",
    "    ## nodes to be active during training\n",
    "    # raise havoc if the limits are not correct\n",
    "    if(limit > self.num_layers or limit < 0):\n",
    "      raise Exception(\"Limits of the network are out of bounds\")\n",
    "    # add a column of ones to take care of the bias only if we do not need the input layer\n",
    "    if((limit == self.num_layers and activation == True) or limit < self.num_layers):\n",
    "      X = np.concatenate((X,np.ones_like(X[:,0])), axis = 1)\n",
    "    # Converting input to matrix form\n",
    "    X = np.mat(X)\n",
    "    \n",
    "    # limits\n",
    "    ## 0 -> -1\n",
    "    ## 1 -> -1\n",
    "    ## 2 -> -2 and so on which generalizes to relu(limit-1)-1\n",
    "    limits = range(self.num_layers-relu(limit-1)-1)\n",
    "    for k in limits:\n",
    "      X = X*np.mat(self.weights[k]) ## Linear\n",
    "      ## if activation=False skip the non-linear part\n",
    "      ## if k is the final element of the limits only then break \n",
    "      ## if limit is not 0, then all the layers must be taken into consideration\n",
    "      if (activation == False and k == limits[-1] and limit != 0):\n",
    "        break\n",
    "      X = sigmoid(X) ## Non-Linear\n",
    "      X = np.concatenate((X,np.ones_like(X[:,0])), axis = 1)\n",
    "      forward_mask, _ = dropout_mask(prob=prob, dim=X.shape[1]-1) ## subtract 1 due to the bias\n",
    "      X = np.multiply(X, forward_mask)\n",
    "      \n",
    "    # if the limit is anything apart the last layer, return the calculated values\n",
    "    if (limit > 0):\n",
    "      return X\n",
    "    \n",
    "    X = X*np.mat(self.weights[-1]) ## Linear\n",
    "    if (activation == True):\n",
    "      return softmax(X) ## Non-Linear\n",
    "    else: \n",
    "      return X\n",
    "  \n",
    "  # implements one iteration of backpropogation through the network\n",
    "  ## it has to use forward() to estimate gradient values at each layer\n",
    "  ## the gradient values for every matrix are returned as a list of matrices\n",
    "  ## which will be used for optimizing the model via stochastic/batch/mini-batch descent\n",
    "  # Note: the gradients are calculated based on the original set of weights provided as X\n",
    "  ## and all the weights ar updated at the end of a complete back prob\n",
    "  # -------------------------------------------------------------------------------\n",
    "  # Variables\n",
    "  # ---------\n",
    "  # grad_W = gradients w.r.t. weights including bias\n",
    "  # grad_H = gradients w.r.t. hidden layers\n",
    "  # grad_A = gradients w.r.t. pre-activation layer\n",
    "  # All of these variables are lists and updated as the backprob traverses through \n",
    "  ## different layers\n",
    "  # -------------------------------------------------------------------------------\n",
    "  def backprob(self, X, gt, prob=1):\n",
    "    self.grad_W = list()\n",
    "    grad_H = list()\n",
    "    grad_A = list()\n",
    "    \n",
    "    ## initializing first gradient\n",
    "    grad_A.append(inv_softmax_with_loss(self.forward(X, prob=prob), gt))\n",
    "    for k in range(self.num_layers):\n",
    "      ## Calculating the forward pass\n",
    "      forward_pass = self.forward(X, k+1, True, prob=prob)\n",
    "      _, backprob_mask = dropout_mask(prob=prob, dim=forward_pass.shape[1]-1) ## subtract 1 due to the bias\n",
    "      masked_input = np.multiply(forward_pass,backprob_mask).T\n",
    "      self.grad_W.append(masked_input * grad_A[k])\n",
    "      ## We do not need the bias layer to update the gradients so we do not keep it\n",
    "      grad_H.append(self.weights[self.num_layers-k-1][:-1,:] * grad_A[k].T) \n",
    "      grad_A.append(np.multiply(grad_H[k].T,inv_sigmoid(self.forward(X,k+1, prob=prob))))\n",
    "\n",
    "    ## reverse the list of gradients as they were stored in the order of backprob\n",
    "    self.grad_W.reverse()\n",
    "    \n",
    "  ## gradient updates which requires learning rate(lr) and momentum(mm)\n",
    "  def grad_descent(self, X, gt, lr=0.1, mm=0, prob=1):\n",
    "    ## calculating gradients\n",
    "    self.backprob(X, gt, prob)\n",
    "    ## updating gradients\n",
    "    for k in range(self.num_layers):\n",
    "      self.v[k] = self.v[k] * mm - lr * self.grad_W[k]\n",
    "      self.weights[k] = self.weights[k] + self.v[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, filename):\n",
    "  fl = open(filename,'wb')\n",
    "  pkl.dump(model,fl)\n",
    "  \n",
    "def load_model(filename):\n",
    "  fl = open(filename, 'rb')\n",
    "  return pkl.load(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = NN([784,100,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.forward(train,  limit=0 ,activation=True, prob=0.5)/ model.forward(train, limit=0,activation= True, prob=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SGD\n",
    "prob=0.5\n",
    "for i in tqdm(range(200)):\n",
    "  for j in range(train.shape[0]):\n",
    "    #print j\n",
    "    model.grad_descent(train[j], train_gt[j], lr=0.01, mm=0.5, prob=prob)\n",
    "    #grad_W_cap = gradient_check(model,train[j], train_gt[j], ep = 0.0001)\n",
    "    #pdb.set_trace()\n",
    "  train_pred = model.forward(train,activation=True, prob=prob)\n",
    "  val_pred = model.forward(val,activation=True, prob=prob)\n",
    "  model.hist.add(cross_entropy_loss(train_pred, train_gt),\n",
    "                cross_entropy_loss(val_pred, val_gt))\n",
    "  print(\"train error: %f, %f\") %(model.hist.train_loss[-1],classification_error(train_pred,train_gt))\n",
    "  print(\"val error: %f, %f\") %(model.hist.val_loss[-1],classification_error(val_pred,val_gt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    train_plt = plt.plot(range(len(model.hist.train_loss)),model.hist.train_loss, 'r--', label='Train')\n",
    "    val_plt = plt.plot(range(len(model.hist.val_loss)),model.hist.val_loss, 'g-', label=\"Val\")\n",
    "    plt.xlabel('No. of Epochs')\n",
    "    plt.ylabel('mean(Entropy Loss)')\n",
    "    plt.savefig('1.png')\n",
    "    #plt.legend([train_plt,val_plt])\n",
    "    #plt.axis([100,200, 0, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Batch Training\n",
    "for i in tqdm(range(200)):\n",
    "  #sample = list(np.floor(np.random.uniform(high=2999,low=0,size=(1000,))).astype(int))\n",
    "  #strain = np.matrix(np.array(train)[sample])\n",
    "  #strain_gt = np.matrix(np.array(train_gt)[sample])\n",
    "  model.grad_descent(train, train_gt, lr = 0.001, prob =1)\n",
    "  #pdb.set_trace()\n",
    "  train_pred = model.forward(train,activation=True, prob=1)\n",
    "  val_pred = model.forward(val,activation=True, prob=1)\n",
    "  model.hist.add(cross_entropy_loss(train_pred, train_gt),\n",
    "                cross_entropy_loss(val_pred, val_gt))\n",
    "  print(\"train error: %f, %f\") %(model.hist.train_loss[-1],classification_error(train_pred,train_gt))\n",
    "  print(\"val error: %f, %f\") %(model.hist.val_loss[-1],classification_error(val_pred,val_gt))\n",
    "  #print(model.weights[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Classification errors\n",
    "print classification_error(model.forward(val, activation=True, prob=1),val_gt)\n",
    "print classification_error(model.forward(test, activation=True, prob=1),test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Visualizing filters\n",
    "plt.imshow(model.weights[0][:-1,12].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## gradient check\n",
    "layer = 0\n",
    "sb.heatmap(model.grad_W[layer] - grad_W_cap[layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1 for i in range(10)] + [0 for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = np.matrix(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.shuffle(b.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = np.matrix([[1,2,3],[4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = np.matrix([[1,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
